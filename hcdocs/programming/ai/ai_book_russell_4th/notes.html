<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>HC Docs</title>
        <style>
        body {
          font-family: sans-serif;
        }

        a {
          text-decoration: none;
          display: inline-block;
          color: #21c;
        }

        a:visited {
          color: #82a;
        }

        a.index {
          margin-bottom: 1rem;
        }

        h2, h3, h4 {
          margin-top: 2.75rem;
        }

        li {
          margin: 0.25rem 0;
        }

        pre {
          box-sizing: border-box;
          margin: 1rem;
          padding: 1rem;
          border: 1px solid #ccc;
          background-color: #ffefd5;
          border-radius: 0.8rem;
          overflow: auto;
          min-width: calc(100% - 2rem);
          font-size: 1rem;
        }

        code {
          padding: 0 0.2rem;
          color: #062;
          font-size: 1rem;
        }

        .hc-monospace {
          font-family: monospace;
        }

        .content {
          max-width: 820px;
          margin-left: auto;
          margin-right: auto;
        }

        table {
          border-collapse: collapse;
        }

        td {
          padding: 0.5rem 0.8rem;
          border: 1px solid #ccc;
        }
        </style>
    </head>
    <body>
      <div style="position: fixed; top: 0; padding: 0.5em; background-color: #fff;">
        <a href="javascript:history.back()">&#x1F519; bk</a>&nbsp;&nbsp;
        <a href="#top">&#x1F51D; top</a>&nbsp;&nbsp;
        <a href="index.html">&#x1F4C2; dir</a>&nbsp;&nbsp;
        <a href="../index.html">&#x23EB; up</a>
      </div>
      <div id="top">&nbsp;</div>
      <div class="content">


    <h3><a href="../../../index.html">home</a> / <a href="../../index.html">programming</a> / <a href="../index.html">ai</a> / <a href="index.html">ai_book_russell_4th</a></h3>
    
<h1>Artificial Intelligence: A Modern Approach, 4th ed.</h1>

<h2 id="ch01">1 Introduction</h2>

<h3>1.1 What is AI?</h3>

<p>
  The six main disciplines of AI are:
</p>

<ul>
  <li>natural language processing</li>
  <li>knowledge representation</li>
  <li>automated reasoning</li>
  <li>machine learning (adapt to new circumstances and extrapolate patterns)</li>
  <li>computer vision</li>
  <li>robotics</li>
</ul>

<h3>1.1.4 Acting rationally (the rational agent approach)</h3>

<p>
  An <strong>agent</strong> is something that acts. A <strong>rational agent</strong> seeks to achieve the best outcome, or the best expected outcome.
</p>

<p>
  The <strong>standard model</strong> is to construct agents that do <em>the right thing</em> (this is defined by the objective given to the agent). It is a kind of optimization problem. In real life, it is rarely possible to attain perfection. <strong>Limited rationality</strong> is acting appropriately when there is not enough time to do all the needed computations.
</p>

<h3>1.1.5 Beneficial machines</h3>

<p>
  The problem of achieving agreement between the creator's preferences and the machine's objective is called the <strong>value alignment problem</strong>. The machine's objectives must be aligned with those of the human creator.
</p>

<h3>1.2.1 Philosophy</h3>

<p>
  John Stuart Mill acknowledged the value of rules, but understood them as efficient decision procedures compiled from first-principles reasoning about consequences. This is an approach adopted by many modern AI systems.
</p>

<h3>1.2.3 Economics</h3>

<p>
  Beyond money, economics is the study of desires and preferences.
</p>

<h3>1.2.4 Neuroscience</h3>

<p>
  There is almost no theory on how an individual memory is stored or on how higher-level cognitive functions operate.
</p>

<h3>1.2.5 Psychology</h3>

<p>
  Doug Engelbart, one of the pioneers of HCI (Human-Computer Interaction) argues that computers should augment human abilities (intelligence augmentation) instead of automating away human tasks.
</p>

<h3>1.2.6 Computer engineering</h3>

<h3>1.2.7 Control theory and cybernetics</h3>

<h3>1.3.8 Deep learning (2011-present)</h3>

<p>
  <strong>Convolutional neural networks</strong> is a used in <strong>deep learning</strong>, and was somewhat successful in handwritten digit recognition.
</p>

<h3>Ch. 1 Summary</h3>

<p>
  Control theory deals with designing devices that act optimally on the basis of feedback from the environment.
</p>

<h2 id="ch02">2 Intelligent Agents</h2>

<h3>2.1 Agents and Environments</h3>

<p>
  An <strong>agent</strong> is anything that can be viewed as perceiving its environment through <strong>sensors</strong> and acting upon the environment through <strong>actuators</strong>.
</p>

<p>
  Interaction: the environment sends or produces percepts that are picked up by the agent's sensors, which are processed to perform actions through actuators.
</p>

<p>
  An agent function is an abstract mathematical description. An agent program is a concrete implementation of an agent function (that runs in a physical system).
</p>

<p>
  Agent example: A vacuum cleaner robot that picks up dust from two squares, A and B, going left or right depending on where it is and the state of the floor (whether there's dirt or not).
</p>

<h3>2.2 Good behavior: The concept of Rationality</h3>

<p>
  <strong>Consequentialism</strong> is evaluating an agent's behavior by its consequences. The desirability of a sequence of actions is captured by a <strong>performance measure</strong>.
</p>

<p>
  In general, it's better to design performance measures according to what one wants, rather than according to how one thinks the agent should behave. For example, if the vacuum cleaner robot were measured by the amount of dust picked up, it could decide to dump it all on the floor and vacuum it up again. It's better to say, "I want a clean floor."
</p>

<p>
  There are still philosophical problems, such as asking which is preferable, an agent that consistently does a mediocre job, or one that cleans energetically but takes long breaks.
</p>

<h3>2.2.2 Rationality</h3>

<p>
  Rationality depends on:
</p>

<ul>
  <li>The performance measure that defines success</li>
  <li>The agent's prior knowledge of the environment</li>
  <li>The agent's percept sequence to date</li>
  <li>The possible actions the agent can perform</li>
</ul>

<p>
  For each possible percept sequence, a rational agent should select an action that is expected to maximize its performance measure, given the evidence provided by the percept sequence and whatever built-in knowledge of the environment.
</p>

<p>
  Going back to the vacuum cleaner, a good agent will stay put once it is sure all squares are clean, and explore (check again) after some time. An irrational agent will oscillate needlessly back and forth if both squares are clean.
</p>

<h3>2.2.3 Omniscience, learning, and autonomy</h3>

<p>
  <strong>Omniscience</strong> means knowing the actual outcome of one's actions, leading to knowing exactly how to act. In practice, it is impossible to be omniscient.
</p>

<p>
  Having <strong>autonomy</strong> means learning on one's own, using one's own percepts and learning processes.
</p>

<h3>2.3.1 Specifying the task environment</h3>

<p>
  The <strong>PEAS</strong> (Performance, Environment, Actuators, Sensors) describes the <strong>task environment</strong>.
</p>

<p>
  Example: autonomous taxi driver
</p>

<table>
  <tr>
    <td>Agent type</td>
    <td>Taxi driver</td>
  </tr>
  <tr>
    <td>Performance measure</td>
    <td>Safe, fast, legal, comfortable, maximize profits, minimize impact on others</td>
  </tr>
  <tr>
    <td>Environment</td>
    <td>Roads, traffic, police, pedestrians, customers, weather</td>
  </tr>
  <tr>
    <td>Actuators</td>
    <td>Accelerator, steering, brakes, signals, horn, speech</td>
  </tr>
  <tr>
    <td>Sensors</td>
    <td>Cameras, radar, speedometer, GPS, engine, accelerometer, microphones, touchscreen</td>
  </tr>
</table>

<h3>2.3.2 Properties of task environments</h3>

<p>
  The dimensions of task environments are:
</p>

<ul>
  <li><strong>Fully observable vs. Partially observable</strong>: Whether sensors can detect the state of the environment completely or not. If the agent does not have sensors, the environment is unobservable.</li>
  <li><strong>Single-agent vs. Multiagent</strong>: Is another object's behavior dependent on the primary agent's actions? Multiagent environments may be competitive or cooperative, or a mix of the two.</li>
  <li><strong>Deterministic vs. Nondeterministic</strong>: The environment is deterministic if the next state of the environment is completely determined by the current state and the action executed by the agent(s). A <strong>stochastic</strong> environment explicitly deals with probabilities (such as a 25% chance of rain), while in a nondeterministic environment, the possibilities are not quantified.</li>
  <li><strong>Episodic vs. Sequential</strong>: In an episodic environment, the agent's experience is divided into atomic episodes. <em>Important</em>: The next episode does not depend on actions taken in previous episodes. For example, classification tasks are episodic. In sequential environments, the current decision could affect all future decisions. Thinking ahead becomes important in these environments.</li>
  <li><strong>Static vs. Dynamic</strong>: A dynamic environment can change while the agent is deliberating (taxi driver). A static environment is easier to deal with because the agent does not need to keep looking at the world while it is deciding on an action (crossword puzzle). An environment is semidynamic if it itself does not change, but agent's performance score does with the passage of time. For example, a game of chess with a timer.</li>
  <li><strong>Discrete vs. Continuous</strong>: The distinctions between these two dimensions apply to the state of the environment, the way time is handled, and the percepts and actions of the agent. A chess environment has a finite number of distinct states (except the clock). On the other hand, taxi driving is a continuous-state and continuous-time problem. Speed, location, and other vehicles sweep through a range of continuous values in a smooth manner. While technically camera input is discrete, it is typically treated as representing continuously varying intensities and locations.</li>
  <li><strong>Known vs. Unknown</strong>: This distinction refers to the agent's (or designer's) state of knowledge about the "laws of physics" of the environment. In an unknown environment, the agent will have to learn how it works in order to make good decisions. In a never seen before video game, rules in the environment could be unknown.</li>
</ul>

<h3>2.4 The structure of agents</h3>

</div>
<br>
<hr>
<div style="padding-left: 0.5em; background-color: #fff;">
  <a href="javascript:history.back()">&#x1F519; Back</a>&nbsp;&nbsp;&nbsp;
  <a href="#top">&#x1F51D; Top</a>&nbsp;&nbsp;&nbsp;
  <a href="index.html">&#x1F4C2; Dir list</a>&nbsp;&nbsp;&nbsp;
  <a href="../index.html">&#x23EB; Up</a>&nbsp;&nbsp;&nbsp;
</div>
<br><br><br>
    </body>
</html>

